{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.1\n",
      "Pandas version: 2.0.3\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.3.0\n",
      "Torch version: 2.0.1+cu117\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA RTX A6000 (cuda)\n",
      "CUDA version: 11.7\n",
      "Number of CUDA devices: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "from category_encoders import BinaryEncoder\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55024792-dddf-4ee4-9dbc-fbcf01ce0c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20170"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=0.001, random_state=42)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc4255-7a42-4172-a91d-4df08665fccc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located (int).\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph:\n",
    "\n",
    "- For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 1,000,000 and greater than 1 (no loops).\n",
    "-  Create a new edge attribute called `LD` for each edge. The value of `LD` is determined by the following formula:\n",
    "     \n",
    "```\n",
    "weights = 1 * e^(-ln(2) / 100_000 * pos_diff_abs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef7f4e-3f36-4b3d-8cf7-6c1902f4747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import time\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import cProfile, pstats, io\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4cc490-c5a0-459f-93ab-fe911ff71239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 20170\n",
      "Number of edges: 145376\n",
      "Node feature dimension: 11\n",
      "Number of isolated nodes: 1436\n",
      "Execution time: 0.5546042919158936 seconds\n",
      "         681059 function calls (679963 primitive calls) in 0.547 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 902 to 3 due to restriction <3>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       15    0.000    0.000    0.558    0.037 C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3472(run_code)\n",
      "       15    0.000    0.000    0.558    0.037 {built-in method builtins.exec}\n",
      "        1    0.204    0.204    0.471    0.471 C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_28224\\2128631100.py:20(preprocess_edges)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edge_weight_cutoff = 1e-3  # set the cutoff value\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "\n",
    "    # Encode categorical columns\n",
    "    categorical_cols = ['ref', 'alt', 'nearest_genes']\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        snp_features[col] = label_encoder.fit_transform(snp_features[col])\n",
    "\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "def preprocess_edges(data: pd.DataFrame, snp_to_idx: dict) -> torch.Tensor:\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    edge_dict = {}  # Use dictionary to store edges and corresponding attributes\n",
    "    raw_edge_attributes = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        # Calculate distance matrix and filter according to conditions\n",
    "        pos_diff = distance.cdist(group[['pos']], group[['pos']], 'cityblock')\n",
    "        mask = (pos_diff > 1) & (pos_diff <= 1_000_000)\n",
    "        \n",
    "        # Filter chunks by mask\n",
    "        rows, cols = np.where(mask)\n",
    "        filtered_chunk = group.iloc[rows]\n",
    "        filtered_chunk_2 = group.iloc[cols]\n",
    "        pos_diff_abs = pos_diff[mask]\n",
    "\n",
    "        weights = np.exp(-np.log(2) / 100_000 * pos_diff_abs)\n",
    "        weight_mask = weights > edge_weight_cutoff\n",
    "\n",
    "        new_edge_list = list(zip(filtered_chunk['snp_idx'][weight_mask], filtered_chunk_2['snp_idx'][weight_mask]))\n",
    "        edge_attr = weights[weight_mask]\n",
    "\n",
    "        for edge, attr in zip(new_edge_list, edge_attr):\n",
    "            sorted_edge = tuple(sorted(edge))  # Sort edge nodes\n",
    "            if sorted_edge not in edge_dict:  # Update dictionary only if edge is not already present\n",
    "                edge_dict[sorted_edge] = attr\n",
    "                raw_edge_attributes.append(attr)\n",
    "\n",
    "    # Fit and apply RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    raw_edge_attributes = np.array(raw_edge_attributes).reshape(-1, 1)\n",
    "    scaled_edge_attributes = scaler.fit_transform(raw_edge_attributes)\n",
    "\n",
    "    edge_list = list(edge_dict.keys())  # Extract edge list from dictionary keys\n",
    "    edge_attributes = list(scaled_edge_attributes.flatten())  # Extract scaled edge attributes\n",
    "\n",
    "    return torch.tensor(edge_list, dtype=torch.long).t().contiguous(), torch.tensor(edge_attributes, dtype=torch.float)\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_attr: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "\n",
    "def count_isolated_nodes(graph: Data) -> int:\n",
    "    node_degrees = graph.edge_index[0].bincount(minlength=graph.num_nodes)\n",
    "    isolated_nodes = (node_degrees == 0).sum().item()\n",
    "    return isolated_nodes\n",
    "\n",
    "# Profiling and execution\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "edges, edge_attr = preprocess_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, edges, edge_attr)\n",
    "graph.y = torch.tensor(data['finemapped'].values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "isolated_nodes = count_isolated_nodes(graph)\n",
    "print(f\"Number of isolated nodes: {isolated_nodes}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6c260d-4a9c-4dd7-bf45-3b9e9c2c8657",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'num_self_loops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\storage.py:79\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\storage.py:104\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'num_self_loops'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of self-loops: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_self_loops\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContains isolated nodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;241m.\u001b[39mcontains_isolated_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContains self-loops: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;241m.\u001b[39mcontains_self_loops()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\data.py:441\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\storage.py:81\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'num_self_loops'"
     ]
    }
   ],
   "source": [
    "print(f\"Number of self-loops: {graph.num_self_loops()}\")\n",
    "print(f\"Contains isolated nodes: {graph.contains_isolated_nodes()}\")\n",
    "print(f\"Contains self-loops: {graph.contains_self_loops()}\")\n",
    "print(f\"Is directed: {graph.is_directed()}\")\n",
    "\n",
    "# Density calculation requires converting to NetworkX graph\n",
    "nx_graph = to_networkx(graph, to_undirected=True)\n",
    "print(f\"Density: {nx.density(nx_graph)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9677fc1-dd2d-4310-9f34-613f05f64ac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorch Geometric -> NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ea21e-4f1c-48b2-93f0-04a71bbc268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def pyg_to_nx(graph):\n",
    "    # Initialize an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Extract edge indices and attributes\n",
    "    edge_index = graph.edge_index.t().tolist()\n",
    "    edge_attr = graph.edge_attr.tolist()\n",
    "\n",
    "    # Combine edge indices and attributes into a single list\n",
    "    edges = [(src, dst, {\"weight\": attr[0]}) for (src, dst), attr in zip(edge_index, edge_attr)]  # Change here: assume the first element of attr is the weight\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Extract node features and create a dictionary with node index as keys\n",
    "    node_features = {i: {f\"feature_{j}\": feat[j] for j in range(len(feat))} for i, feat in enumerate(graph.x.tolist())}  # Change here: convert list of lists to dict\n",
    "\n",
    "    # Set node attributes\n",
    "    nx.set_node_attributes(G, node_features)\n",
    "    \n",
    "    return G\n",
    "\n",
    "nx_graph = pyg_to_nx(graph)\n",
    "\n",
    "# Print the total number of nodes and edges in the graph\n",
    "print(f\"Number of nodes: {nx_graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {nx_graph.number_of_edges()}\")\n",
    "\n",
    "# Calculate average degree\n",
    "degrees = [degree for _, degree in nx_graph.degree()]\n",
    "average_degree = sum(degrees) / nx_graph.number_of_nodes()\n",
    "print(f\"Average degree: {average_degree}\")\n",
    "\n",
    "\n",
    "# Calculate and print the number of connected components\n",
    "num_connected_components = nx.number_connected_components(nx_graph)\n",
    "print(f\"Number of connected components: {num_connected_components}\")\n",
    "\n",
    "# Find and print the largest connected component\n",
    "largest_connected_component = max(nx.connected_components(nx_graph), key=len)\n",
    "print(f\"Largest connected component (number of nodes): {len(largest_connected_component)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a97ab-d8a0-4867-982a-11f809d38719",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Louvain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8a4c9-e9de-4192-be8e-b24b2cdc882d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Detect communities using Louvain method\n",
    "partition = community_louvain.best_partition(nx_graph, weight='weight')\n",
    "\n",
    "# Print number of communities\n",
    "print(f\"Number of communities: {len(set(partition.values()))}\")\n",
    "\n",
    "# Create a reverse mapping from community ID to nodes\n",
    "community_to_nodes = {}\n",
    "for node, community_id in partition.items():\n",
    "    if community_id not in community_to_nodes:\n",
    "        community_to_nodes[community_id] = []\n",
    "    community_to_nodes[community_id].append(node)\n",
    "\n",
    "# Print descriptive statistics for each feature per community\n",
    "for community_id, nodes in community_to_nodes.items():\n",
    "    print(f\"Community {community_id}:\")\n",
    "\n",
    "    # Gather features for nodes in this community\n",
    "    features = [nx_graph.nodes[node] for node in nodes]\n",
    "    \n",
    "    # Separate features into separate lists for easier statistics computation\n",
    "    # (assuming node features are stored in the format `{'feature_0': value, 'feature_1': value, ...}`)\n",
    "    separated_features = {}\n",
    "    for node_features in features:\n",
    "        for feature_name, feature_value in node_features.items():\n",
    "            if feature_name not in separated_features:\n",
    "                separated_features[feature_name] = []\n",
    "            separated_features[feature_name].append(feature_value)\n",
    "    \n",
    "    # Print descriptive statistics for each feature\n",
    "    for feature_name, feature_values in separated_features.items():\n",
    "        print(f\"  Feature {feature_name}:\")\n",
    "        print(f\"    Mean: {np.mean(feature_values)}\")\n",
    "        print(f\"    Standard deviation: {np.std(feature_values)}\")\n",
    "        print(f\"    Min: {np.min(feature_values)}\")\n",
    "        print(f\"    Max: {np.max(feature_values)}\")\n",
    "        print(f\"    Median: {np.median(feature_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261b811-c831-40af-820f-af03ad56778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a9ef4-b3c2-4670-be6c-9375f0bea019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "clusters = kmeans.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a133b-9855-4056-bd05-30106d3e7e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb8e86-0c9e-463a-a2c2-57204f9c28a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "clusters = hc.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e556b25-5e86-4a6b-be7d-ccd72c441f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fae162-d43d-4681-8f74-d25fb32b6b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "clusters = dbscan.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48037339-5dbd-41b4-b4d6-63308217d087",
   "metadata": {
    "tags": []
   },
   "source": [
    "### grid_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8d7f2-4a15-472e-8135-512b4c1fd2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_cluster import grid_cluster\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assume pos_chrom is a tensor of shape [num_nodes, 2], \n",
    "# where the first column represents 'pos' and the second column represents '#chrom'\n",
    "pos_chrom = torch.tensor(np.stack([snp_features['pos'].values, snp_features['#chrom'].values])).t()\n",
    "\n",
    "# Define the size of the grid cells. You may want to adjust this according to your needs.\n",
    "size = torch.tensor([2e7, 1])  # Creates a grid with cell size defined according to your data distribution\n",
    "\n",
    "# Perform the grid clustering\n",
    "cluster = grid_cluster(pos_chrom, size)\n",
    "\n",
    "# Now, cluster is a tensor of size [num_nodes] where each element is the cluster index of the corresponding node\n",
    "print(cluster)\n",
    "\n",
    "# Get the 'pos' and '#chrom' data\n",
    "x = pos_chrom[:, 0].numpy()  # extract 'pos'\n",
    "y = pos_chrom[:, 1].numpy()  # extract '#chrom'\n",
    "\n",
    "# Convert cluster tensor to numpy for use with matplotlib\n",
    "cluster = cluster.numpy()\n",
    "\n",
    "num_clusters = len(np.unique(cluster))\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "# Create a colormap based on the number of unique clusters\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list('rainbow', plt.cm.rainbow(np.linspace(0, 1, len(np.unique(cluster)))))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(x, y, c=cluster, cmap=cmap, alpha=0.6)\n",
    "plt.colorbar(sc, label='Cluster Index')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Chromosome Number')\n",
    "plt.title('Grid Clustering of SNPs')\n",
    "\n",
    "# Modify y-ticks to represent actual chromosome numbers\n",
    "plt.yticks(range(1, int(y.max())+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9ec22-c065-4cc1-b86b-a75ffe3de3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
