{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.1\n",
      "Pandas version: 1.5.3\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.3.0\n",
      "Torch version: 2.0.1+cu117\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA RTX A6000 (cuda)\n",
      "CUDA version: 11.7\n",
      "Number of CUDA devices: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "from category_encoders import BinaryEncoder\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64',\n",
    "    'causal': 'int64',\n",
    "    'trait': 'string'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('gwas-fine-causal.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'causal', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64',\n",
    "    'causal': 'int64',\n",
    "    'trait': 'string'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "causal                   0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55024792-dddf-4ee4-9dbc-fbcf01ce0c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20170006"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = data.sample(frac=0.05, random_state=42)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc4255-7a42-4172-a91d-4df08665fccc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located (int).\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph:\n",
    "\n",
    "- For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 1,000,000 and greater than 1 (no loops). Create edges between all pairs of SNPs within the 1,000,000 base distance threshold. The edge weight is determined by the following formula:\n",
    "     \n",
    "```\n",
    "weights = 1 * e^(-ln(2) / 100_000 * pos_diff_abs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# pyg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e92024-7721-4389-b5c4-78ad7419c983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## pyg creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef7f4e-3f36-4b3d-8cf7-6c1902f4747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import time\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import cProfile, pstats, io\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4ded9f9-9d64-4791-af5d-2713c7535b3d",
   "metadata": {},
   "source": [
    "from math import exp, log\n",
    "\n",
    "edge_weight_cutoff = 1e-3  # set the cutoff value\n",
    "\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    categorical_cols = ['ref', 'alt', 'nearest_genes']\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    #snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "    \n",
    "\n",
    "def preprocess_edges(data: pd.DataFrame, snp_to_idx: dict, chunk_size: int=100_000) -> torch.Tensor:\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['nearest_genes'] = label_encoder.fit_transform(data['nearest_genes'])\n",
    "\n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        for i in range(0, len(group), chunk_size):\n",
    "            chunk = group.iloc[i: i + chunk_size]\n",
    "\n",
    "            # Broadcasting to create a matrix of differences\n",
    "            pos_diff = abs(chunk['pos'].values[:, None] - chunk['pos'].values)\n",
    "            mask = (pos_diff > 1) & (pos_diff <= 1_000_000)\n",
    "\n",
    "            filtered_chunk = chunk[mask]\n",
    "\n",
    "            if filtered_chunk.empty:\n",
    "                continue\n",
    "\n",
    "            new_edge_list = [(idx, other_idx) for idx, others in enumerate(mask) for other_idx in others.nonzero()[0]]\n",
    "            edge_list.extend(new_edge_list)\n",
    "\n",
    "            edge_attr = np.vstack([\n",
    "                np.exp(-np.log(2) / 100_000 * pos_diff[mask])\n",
    "            ]).T.astype(np.float32)\n",
    "\n",
    "            edge_attributes.extend(edge_attr.tolist())\n",
    "\n",
    "    edge_list = np.array(edge_list, dtype=int)\n",
    "    edge_attributes = scaler.fit_transform(np.array(edge_attributes, dtype=np.float32))\n",
    "\n",
    "    # Remove edges with negative weights\n",
    "    mask = edge_attributes >= edge_weight_cutoff\n",
    "    edge_list = edge_list[mask.reshape(-1), :]\n",
    "    edge_attributes = edge_attributes[mask]\n",
    "\n",
    "    return torch.tensor(edge_list, dtype=torch.long).t().contiguous(), torch.tensor(edge_attributes, dtype=torch.float)\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_attr: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "def count_isolated_nodes(graph: Data) -> int:\n",
    "    node_degrees = graph.edge_index[0].bincount(minlength=graph.num_nodes)\n",
    "    isolated_nodes = (node_degrees == 0).sum().item()\n",
    "    return isolated_nodes\n",
    "\n",
    "\n",
    "# Create a profiler object\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "edges, edge_attr = preprocess_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, edges, edge_attr)\n",
    "graph.y = torch.tensor(data['finemapped'].values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "isolated_nodes = count_isolated_nodes(graph)\n",
    "print(f\"Number of isolated nodes: {isolated_nodes}\")\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 3 lines\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df22b92-01b5-4225-814d-ad95b11b875a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b5640e1-b322-4c4f-96ed-f39654c78729",
   "metadata": {},
   "source": [
    "from torch_geometric.utils import degree\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_graph_stats(graph, features_list):\n",
    "    print(f\"Number of nodes: {graph.x.size(0)}\")\n",
    "    print(f\"Number of edges: {graph.edge_index.size(1)}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "    # Compute and print degree-related stats\n",
    "    degrees = degree(graph.edge_index[0].long(), num_nodes=graph.x.size(0))\n",
    "    average_degree = degrees.float().mean().item()\n",
    "    median_degree = np.median(degrees.numpy())\n",
    "    std_degree = degrees.float().std().item()\n",
    "\n",
    "    print(f\"Average Degree: {average_degree}\")\n",
    "    print(f\"Median Degree: {median_degree}\")\n",
    "    print(f\"Standard Deviation of Degree: {std_degree}\")\n",
    "\n",
    "    # Compute and print edge-related stats\n",
    "    average_edge_weight = graph.edge_attr.float().mean().item()\n",
    "    median_edge_weight = np.median(graph.edge_attr.numpy())\n",
    "    std_edge_weight = graph.edge_attr.float().std().item()\n",
    "    min_edge_weight = graph.edge_attr.float().min().item()\n",
    "    max_edge_weight = graph.edge_attr.float().max().item()\n",
    "\n",
    "    print(f\"Average Edge Weight: {average_edge_weight}\")\n",
    "    print(f\"Median Edge Weight: {median_edge_weight}\")\n",
    "    print(f\"Standard Deviation of Edge Weight: {std_edge_weight}\")\n",
    "    print(f\"Min Edge Weight: {min_edge_weight}\")\n",
    "    print(f\"Max Edge Weight: {max_edge_weight}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.x.size(0) * (graph.x.size(0) - 1) / 2\n",
    "    density = graph.edge_index.size(1) / num_possible_edges\n",
    "\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_mask = torch.isnan(graph.x)\n",
    "    nan_features = []\n",
    "    for feature_idx, feature_name in enumerate(features_list):\n",
    "        if nan_mask[:, feature_idx].any():\n",
    "            nan_features.append(feature_name)\n",
    "\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_features)\n",
    "\n",
    "    # Compute and print descriptive stats for node feature vectors\n",
    "    feature_stats = []\n",
    "    node_features = graph.x\n",
    "    for i, feature_name in enumerate(features_list):\n",
    "        feature = node_features[:, i]\n",
    "        mean = feature.float().mean().item()\n",
    "        median = np.median(feature.numpy())\n",
    "        std = feature.float().std().item()\n",
    "        minimum = feature.float().min().item()\n",
    "        maximum = feature.float().max().item()\n",
    "\n",
    "        feature_stats.append([feature_name, mean, median, std, minimum, maximum])\n",
    "\n",
    "    headers = [\"Feature\", \"Mean\", \"Median\", \"Standard Deviation\", \"Minimum\", \"Maximum\"]\n",
    "    print(\"\\nNode Feature Vector Descriptive Statistics:\")\n",
    "    print(tabulate(feature_stats, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Print graph stats\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, snp_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fff486-f6da-460d-8554-280e66b1ece1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# nx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9e78a-5503-4019-9208-46fcf26451c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## nx creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a6ced3-1a94-4280-b3ec-7363bd5710d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance\n",
    "import numba\n",
    "import cupy as cp\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f6048-4750-46e0-8691-c641358dc3bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPU-enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ab1ff5-c9df-4cb0-8725-28faedfbc019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 35min 43s\n",
      "Wall time: 36min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_graph(data):\n",
    "    data.sort_values(['#chrom', 'pos'], inplace=True)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    nodes = data.set_index('id')[['nearest_genes', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']].to_dict('index')\n",
    "    G.add_nodes_from(nodes.items())\n",
    "\n",
    "    def calculate_weights(pos_diffs):\n",
    "        mask = (pos_diffs > 1) & (pos_diffs <= 300_000)\n",
    "        indices = cp.argwhere(mask)  # indices where condition holds\n",
    "        unique_indices = indices[indices[:, 0] < indices[:, 1]]  # indices where first index < second index\n",
    "        if unique_indices.size > 0:  # check if there are unique_indices\n",
    "            unique_pos_diffs = pos_diffs[unique_indices[:, 0], unique_indices[:, 1]]\n",
    "            return unique_indices, cp.exp(-cp.log(2) / 100_000 * unique_pos_diffs)\n",
    "        else:\n",
    "            return cp.array([]), cp.array([])\n",
    "\n",
    "    # Divide the data into 2 halves for multi-GPU computation\n",
    "    halves = [data[data['#chrom'] <= data['#chrom'].median()], data[data['#chrom'] > data['#chrom'].median()]]\n",
    "    \n",
    "    for device, data_half in enumerate(halves):\n",
    "        with cp.cuda.Device(device):  # Specify the device\n",
    "            for chrom, group in data_half.groupby('#chrom'):\n",
    "                ids = group['id'].values\n",
    "                pos = cp.asarray(group['pos'].values, dtype=cp.float32)  # Use float32 for reduced memory\n",
    "                \n",
    "                chunk_size = 37_000\n",
    "                overlap = 2_000 \n",
    "                num_chunks = math.ceil(len(pos) / chunk_size)\n",
    "                \n",
    "                for chunk in range(num_chunks):\n",
    "                    start_idx = max(0, chunk * chunk_size - overlap)\n",
    "                    end_idx = min((chunk + 1) * chunk_size + overlap, len(pos))\n",
    "                    \n",
    "                    chunk_pos = pos[start_idx:end_idx]\n",
    "                    chunk_pos_diffs = cp.empty((len(chunk_pos), len(chunk_pos)), dtype=cp.float32)\n",
    "                    chunk_pos_diffs -= chunk_pos[:, None]  # Compute difference in-place\n",
    "                    chunk_pos_diffs = cp.abs(chunk_pos_diffs)\n",
    "\n",
    "                    unique_indices, unique_weights = calculate_weights(chunk_pos_diffs)\n",
    "                    unique_weights = cp.asnumpy(unique_weights)\n",
    "                    unique_indices = cp.asnumpy(unique_indices)  # conversion to NumPy array\n",
    "\n",
    "                    if unique_indices.size > 0:\n",
    "                        edges = [(ids[unique_indices[i, 0]], ids[unique_indices[i, 1]], unique_weights[i]) for i in range(unique_indices.shape[0])]\n",
    "                        G.add_weighted_edges_from(edges)\n",
    "\n",
    "                del ids, pos\n",
    "                cp.cuda.Stream.null.synchronize()\n",
    "                cp._default_memory_pool.free_all_blocks()\n",
    "                group = None\n",
    "\n",
    "    return G\n",
    "\n",
    "nx_graph = create_graph(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58647c8-f988-4e09-a6d4-3c77545aaaa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CPU-only"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c976ec3b-7a94-463f-9c92-b1e4d3e89520",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "def create_graph(data):\n",
    "    # Sort the data\n",
    "    data.sort_values(['#chrom', 'pos'], inplace=True)\n",
    "    \n",
    "    # Create the graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph in bulk using a dict comprehension\n",
    "    G.add_nodes_from({\n",
    "        row['id']: row.to_dict() \n",
    "        for _, row in data.iterrows()\n",
    "    })\n",
    "\n",
    "    # Define a function to calculate weights for given indices\n",
    "    def calculate_weights(pos_diffs):\n",
    "        mask = (pos_diffs > 1) & (pos_diffs <= 300_000)\n",
    "        indices = np.argwhere(mask)\n",
    "        unique_indices = indices[indices[:, 0] < indices[:, 1]]\n",
    "        unique_pos_diffs = pos_diffs[unique_indices[:, 0], unique_indices[:, 1]]\n",
    "        return unique_indices, 1 * np.exp(-np.log(2) / 100_000 * unique_pos_diffs)\n",
    "\n",
    "    # Iterate over each chromosome group\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        ids = group['id'].values\n",
    "        pos = group['pos'].values\n",
    "        \n",
    "        # Apply batch operation\n",
    "        chunk_size = 40_000\n",
    "        overlap = 2_500  # Define overlap size\n",
    "        num_chunks = math.ceil(len(pos) / chunk_size)\n",
    "        \n",
    "        for chunk in range(num_chunks):\n",
    "            start_idx = max(0, chunk * chunk_size - overlap)\n",
    "            end_idx = min((chunk + 1) * chunk_size + overlap, len(pos))\n",
    "            \n",
    "            # Calculate pairwise absolute differences in position within each chunk\n",
    "            chunk_pos = pos[start_idx:end_idx]\n",
    "            chunk_pos_diffs = distance.squareform(distance.pdist(chunk_pos[:, None], 'cityblock'))\n",
    "\n",
    "            # Calculate weights\n",
    "            unique_indices, unique_weights = calculate_weights(chunk_pos_diffs)\n",
    "\n",
    "            # Add the unique edges to the graph with weights\n",
    "            for (i, j), weight in zip(unique_indices, unique_weights):\n",
    "                node1 = ids[start_idx + i]\n",
    "                node2 = ids[start_idx + j]\n",
    "                if not G.has_edge(node1, node2):\n",
    "                    G.add_edge(node1, node2, weight=weight)\n",
    "\n",
    "        del ids, pos  # Delete to free memory\n",
    "        group = None  # Free memory\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "nx_graph = create_graph(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d6bea-d7b6-4e6d-86bf-60b71bc22c77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Write nx "
   ]
  },
  {
   "cell_type": "raw",
   "id": "baecadfc-60e1-4730-bc07-2774e7388358",
   "metadata": {},
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open('nx_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(nx_graph, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd369371-2e5a-4075-a1cf-f197df7a0300",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Read nx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88c1ac01-3374-45dd-b4c4-33d02f19ab23",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('nx_graph.pkl', 'rb') as f:\n",
    "    nx_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595678bf-6de5-42a5-9aa1-2095ad0af78a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## nx stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6002583e-edf3-4e90-8af0-f1e08f2c9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 20170006\n",
      "Number of edges: 1065156421\n",
      "Number of connected components: 19543996\n",
      "Degree distribution statistics:\n",
      "Minimum degree: 0\n",
      "Maximum degree: 38999\n",
      "Average degree: 3460.0434782608695\n",
      "Edge weight statistics:\n",
      "Minimum weight: 0.12500519871195762\n",
      "Maximum weight: 0.9998267282181494\n",
      "Average weight: 0.3296916121860768\n",
      "Size of the largest connected component (nodes): 39000\n",
      "Size of the largest connected component (edges): 1045025415\n",
      "CPU times: total: 13min 58s\n",
      "Wall time: 26min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Print basic graph statistics\n",
    "num_nodes = nx_graph.number_of_nodes()\n",
    "num_edges = nx_graph.number_of_edges()\n",
    "\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)\n",
    "\n",
    "# Print number of connected components\n",
    "num_components = nx.number_connected_components(nx_graph)\n",
    "print(\"Number of connected components:\", num_components)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Get degree distribution\n",
    "degree_sequence = sorted([d for n, d in nx_graph.degree()], reverse=True)\n",
    "degree_counts = Counter(degree_sequence)\n",
    "\n",
    "# Print degree distribution statistics\n",
    "degrees = np.array(list(degree_counts.keys()))\n",
    "print(\"Degree distribution statistics:\")\n",
    "print(\"Minimum degree:\", degrees.min())\n",
    "print(\"Maximum degree:\", degrees.max())\n",
    "print(\"Average degree:\", degrees.mean())\n",
    "\n",
    "# Calculate edge weight statistics\n",
    "edge_sum = 0\n",
    "edge_count = 0\n",
    "min_weight = float('inf')\n",
    "max_weight = float('-inf')\n",
    "\n",
    "for _, _, data in nx_graph.edges(data=True):\n",
    "    if 'weight' in data:\n",
    "        weight = float(data['weight'])\n",
    "        edge_sum += weight\n",
    "        edge_count += 1\n",
    "        min_weight = min(min_weight, weight)\n",
    "        max_weight = max(max_weight, weight)\n",
    "\n",
    "average_weight = edge_sum / edge_count if edge_count > 0 else float('nan')\n",
    "\n",
    "# Print edge weight statistics\n",
    "print(\"Edge weight statistics:\")\n",
    "print(\"Minimum weight:\", min_weight)\n",
    "print(\"Maximum weight:\", max_weight)\n",
    "print(\"Average weight:\", average_weight)\n",
    "\n",
    "# Find the largest connected component\n",
    "largest_component = max(nx.connected_components(nx_graph), key=len)\n",
    "\n",
    "# Print the size of the largest connected component\n",
    "print(\"Size of the largest connected component (nodes):\", len(largest_component))\n",
    "print(\"Size of the largest connected component (edges):\", num_edges - (num_nodes - len(largest_component)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e4d908-2695-417a-8b7b-cbc0cd1b3ec1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "051e9dda-10bb-41b0-85c2-98df5e27e817",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from community import community_louvain\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dbe45-93d8-403f-a446-0e0d033e6566",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Louvain Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bccbf2e0-0483-45b5-b910-98cc231f935f",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%time\n",
    "\n",
    "# Detect communities using Louvain method\n",
    "partition = community_louvain.best_partition(nx_graph, weight='weight')\n",
    "\n",
    "# Print number of communities\n",
    "print(f\"Number of communities: {len(set(partition.values()))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5786c1-c6f1-4724-b0f9-16c57c587d5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Community Size Histogram"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2472b233-992c-46e0-8193-587ede726b99",
   "metadata": {},
   "source": [
    "communities = set(partition.values())\n",
    "community_sizes = [list(partition.values()).count(x) for x in communities]\n",
    "\n",
    "plt.hist(community_sizes, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Community Size Distribution')\n",
    "plt.xlabel('Community Size')\n",
    "plt.ylabel('Number of Communities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c19110-c924-43a3-8ab0-bdf02cb70bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Degree Distribution per Community"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c6dfc5d-071e-4e5c-bc7f-c73696801c55",
   "metadata": {},
   "source": [
    "communities = defaultdict(list)\n",
    "\n",
    "for node, community in partition.items():\n",
    "    communities[community].append(node)\n",
    "plt.figure(figsize=(10,6))\n",
    "colors = sns.color_palette('pastel')[0:len(communities)]\n",
    "\n",
    "for (community, nodes), color in zip(communities.items(), colors):\n",
    "    degrees = [nx_graph.degree(n) for n in nodes]\n",
    "    sns.histplot(degrees, bins=10, kde=False, color=color, element=\"step\", stat=\"density\")\n",
    "\n",
    "plt.title('Degree Distribution per Community')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6731da3-b0c1-4a83-8912-f5f062b6b700",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Centrality measures per community"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76ccf967-3a46-4b44-93aa-d9475d178a41",
   "metadata": {},
   "source": [
    "# Calculating centrality measures for large graphs can be computationally expensive.\n",
    "# Therefore, you might want to select a subset of communities or nodes for this analysis.\n",
    "for community, nodes in list(communities.items())[:5]:  # Just the first 5 communities\n",
    "    subgraph = nx_graph.subgraph(nodes)\n",
    "    degree_centrality = nx.degree_centrality(subgraph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "    closeness_centrality = nx.closeness_centrality(subgraph)\n",
    "\n",
    "    print(f'Community {community}:')\n",
    "    print(f'Max degree centrality: {max(degree_centrality.items(), key=operator.itemgetter(1))}')\n",
    "    print(f'Max betweenness centrality: {max(betweenness_centrality.items(), key=operator.itemgetter(1))}')\n",
    "    print(f'Max closeness centrality: {max(closeness_centrality.items(), key=operator.itemgetter(1))}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f5919-775f-4677-bc73-61acf492aab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Community interconnectivity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9a468d9-6a73-468b-937c-b0280dc2177f",
   "metadata": {},
   "source": [
    "intra_edges = 0\n",
    "inter_edges = 0\n",
    "for u, v in nx_graph.edges():\n",
    "    if partition[u] == partition[v]:\n",
    "        intra_edges += 1\n",
    "    else:\n",
    "        inter_edges += 1\n",
    "\n",
    "print(f'Ratio of intra-community to inter-community edges: {intra_edges / inter_edges}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70dafc1-305a-4ba7-b437-1e38e26fea56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Community contribution to overall network structure"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a49644c-6ec1-4073-b4b8-3d6f75fd4581",
   "metadata": {},
   "source": [
    "communities = defaultdict(list)\n",
    "\n",
    "for node, community in partition.items():\n",
    "    communities[community].append(node)\n",
    "    \n",
    "community_edge_weights = dict()\n",
    "for community, nodes in communities.items():\n",
    "    subgraph = nx_graph.subgraph(nodes)\n",
    "    total_edge_weight = sum([data.get('weight', 1) for u, v, data in subgraph.edges(data=True)])\n",
    "    community_edge_weights[community] = total_edge_weight\n",
    "\n",
    "# Convert dictionary to DataFrame for better visualization and sort by the edge weight\n",
    "df = pd.DataFrame(list(community_edge_weights.items()), columns=['Community', 'Sum of edge weights'])\n",
    "df = df.sort_values('Sum of edge weights', ascending=False)\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6135f4-61c9-4da4-b386-df008596d234",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Community chrom and min-max pos identification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05cf58d0-2749-4f87-8f56-e455ecbd72c7",
   "metadata": {},
   "source": [
    "# Create a dictionary for storing the chromosome and min and max position for each community\n",
    "community_chrom_pos = dict()\n",
    "\n",
    "for community, nodes in communities.items():\n",
    "    # Create a subgraph for the community\n",
    "    subgraph = nx_graph.subgraph(nodes)\n",
    "    # Get the rows in the original data corresponding to the nodes in this community\n",
    "    community_data = data[data['id'].isin(nodes)]\n",
    "    # Get the chromosome(s) and position range for these rows\n",
    "    chromosomes = community_data['#chrom'].unique()\n",
    "    min_pos = community_data['pos'].min()\n",
    "    max_pos = community_data['pos'].max()\n",
    "    # Store this information in the dictionary\n",
    "    community_chrom_pos[community] = {'chromosomes': chromosomes, 'min_pos': min_pos, 'max_pos': max_pos}\n",
    "\n",
    "# Convert this dictionary to a DataFrame for better visualization\n",
    "df_chrom_pos = pd.DataFrame(community_chrom_pos).T\n",
    "\n",
    "def print_community_info(community_number, community_chrom_pos):\n",
    "    if community_number in community_chrom_pos:\n",
    "        info = community_chrom_pos[community_number]\n",
    "        print(f\"Community {community_number} info:\")\n",
    "        print(f\"Chromosomes: {info['chromosomes']}\")\n",
    "        print(f\"Min position: {info['min_pos']}\")\n",
    "        print(f\"Max position: {info['max_pos']}\")\n",
    "    else:\n",
    "        raise ValueError(f\"No community with number {community_number}.\")\n",
    "\n",
    "# Print info for community 5\n",
    "print_community_info(145, community_chrom_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78d1d0-5324-49ad-8f12-b46ea64cc843",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Subgraph visualization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ecfcf4a-eb54-4b69-b68b-2b767be17fb7",
   "metadata": {},
   "source": [
    "# 5. \n",
    "# Select the first community for demonstration. You might want to select different or multiple communities for your analysis.\n",
    "community_to_draw = list(communities.keys())[0]  \n",
    "nodes = communities[community_to_draw]\n",
    "subgraph = nx_graph.subgraph(nodes)\n",
    "\n",
    "pos = nx.spring_layout(subgraph)\n",
    "nx.draw(subgraph, pos, with_labels=False, node_color='blue', node_size=30, edge_color='grey')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261b811-c831-40af-820f-af03ad56778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### k-clique communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8268bdb-014c-4370-a952-07258e25fbdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Girvan-Newman"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f420e62-4b0f-4724-b61c-e57384fcb7c7",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "import networkx.algorithms.community as nxcom\n",
    "\n",
    "# Apply Girvan-Newman community detection\n",
    "communities_generator = nxcom.girvan_newman(nx_graph)\n",
    "\n",
    "# Print the first k tuples of communities\n",
    "k = 2\n",
    "for communities in itertools.islice(communities_generator, k):\n",
    "    print(f\"Communities: {tuple(sorted(c) for c in communities)}The `girvan_newman` function from NetworkX provides a community detection algorithm that partitions the input graph. It returns an iterator over tuples of sets of nodes in `G`. Each set of nodes is a community, and each tuple is a sequence of communities at a particular level of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072113be-595d-4f3e-a9c1-5784a4335d38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Label Propagation Algorithm (LPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0412b-91a0-4f4a-8b47-ef12c8b24c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Leading Eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7afaa3-d23c-4672-93e4-82ed1fc3b968",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Walktrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b225e9-9a2b-4778-91aa-332ff1811b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Edge Betweenness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91aa4db-f8d1-4786-bebe-bc8c15e9bd3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670fd17-c08c-4945-b29e-842d9c597406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da81c9e8-905e-41dc-aef6-284175b857f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# nk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf51d5e-ae62-459f-a1d0-6f03d415e539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import networkit as nk\n",
    "from scipy.spatial import distance\n",
    "import numba\n",
    "import cupy as cp\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996f0aa-f43f-4a45-a0f9-b687839b6d87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## nx -> nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08eae86a-770e-43d6-90f7-0717bc5c8074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 28s\n",
      "Wall time: 13min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nk_graph = nk.nxadapter.nx2nk(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43996e25-752c-479a-97e6-747b1b0d4daf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddc2cf03-a05a-4bdb-9306-46e3de896801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:overriding given file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 5s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Assuming nk_graph has been converted from a networkx graph\n",
    "nk.writeGraph(nk_graph, 'nk_graph.networkit', nk.Format.NetworkitBinary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c031da-6445-4bd2-a7b6-278ab9e7628b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39ef32-d8ad-484d-a371-36e78f135411",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Assuming nk_graph has been written to a GraphML file\n",
    "nk_graph = nk.readGraph('nk_graph.networkit', nk.Format.NetworkitBinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c269c46-0aef-4d7b-874c-ee55c46d3217",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9dd05-7ac8-4008-b5af-2b59f2a587d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PLM (Parallel Louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a178db13-7777-4dc5-86fd-e55f49174aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkit\\stopwatch.py:49: UserWarning: networkit.Timer is deprecated, will be removed in future updates.\n",
      "  warn(\"networkit.Timer is deprecated, will be removed in future updates.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities detected in 17.56860 [s]\n",
      "solution properties:\n",
      "-------------------  --------------\n",
      "# communities            1.9544e+07\n",
      "min community size       1\n",
      "max community size   39000\n",
      "avg. community size      1.03203\n",
      "imbalance            19500\n",
      "edge cut                 0\n",
      "edge cut (portion)       0\n",
      "modularity               0.926649\n",
      "-------------------  --------------\n",
      "20170006 elements assigned to 19543996 subsets\n",
      "the biggest subset has size 39000\n",
      "CPU times: total: 3min 52s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Choose and initialize algorithm\n",
    "plmCommunities = nk.community.detectCommunities(nk_graph, algo=nk.community.PLM(nk_graph, True))\n",
    "\n",
    "print(\"{0} elements assigned to {1} subsets\".format(plmCommunities.numberOfElements(), plmCommunities.numberOfSubsets()))\n",
    "print(\"the biggest subset has size {0}\".format(max(plmCommunities.subsetSizes())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddf09b1-8863-443c-9d27-0439ef2916bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Find index of largest subset\n",
    "max_size = max(plmCommunities.subsetSizes())\n",
    "max_index = plmCommunities.subsetSizes().index(max_size)\n",
    "\n",
    "print(max_size)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad22ae-142c-44fb-b3d5-5ea64ca1d7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
