{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.3\n",
      "Pandas version: 2.0.1\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.2.2\n",
      "Torch version: 2.0.0+cu118\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA GeForce RTX 3060 Ti (cuda)\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "from category_encoders import BinaryEncoder\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c895d44b-a5d7-43eb-bbb4-7138312b49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc4255-7a42-4172-a91d-4df08665fccc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55024792-dddf-4ee4-9dbc-fbcf01ce0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located (int).\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph:\n",
    "\n",
    "- For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 1,000,000 and greater than 1 (no loops).\n",
    "-  Create a new edge attribute called `LD` for each edge. The value of `LD` is determined by the following formula:\n",
    "     \n",
    "```\n",
    "weights = 1 * e^(-ln(2) / 100_000 * pos_diff_abs)\n",
    "```\n",
    "\n",
    "- The edge attributes (`edge_attr`) are:\n",
    "   - `nearest_genes` \n",
    "   - `#chrom`\n",
    "   - `pos`\n",
    "   - `af_alt`, \n",
    "   - `af_alt_cases` \n",
    "   - `af_alt_controls`\n",
    "   - `LD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c632e38-0f05-4729-a7d5-df625d0b26ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4cc490-c5a0-459f-93ab-fe911ff71239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 100850\n",
      "Number of edges: 7284022\n",
      "Node feature dimension: 11\n",
      "Number of isolated nodes: 92636\n",
      "Execution time: 9.31654143333435 seconds\n",
      "         3925058 function calls (3923667 primitive calls) in 9.168 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1059 to 3 due to restriction <3>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       15    0.000    0.000    9.316    0.621 C:\\Users\\falty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3472(run_code)\n",
      "       15    0.000    0.000    9.316    0.621 {built-in method builtins.exec}\n",
      "        1    2.215    2.215    7.858    7.858 C:\\Users\\falty\\AppData\\Local\\Temp\\ipykernel_21964\\3294778173.py:31(preprocess_edges)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import exp, log\n",
    "\n",
    "edge_weight_cutoff = 1e-3  # set the cutoff value\n",
    "\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    categorical_cols = ['ref', 'alt', 'nearest_genes']\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    #snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_edges(data: pd.DataFrame, snp_to_idx: dict) -> torch.Tensor:\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['nearest_genes'] = label_encoder.fit_transform(data['nearest_genes'])\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        if group.empty:\n",
    "            continue\n",
    "        \n",
    "        # We can use broadcasting to create a matrix of differences, this avoids the inner loop\n",
    "        pos_diff = abs(group['pos'].values[:, None] - group['pos'].values)\n",
    "        mask = (pos_diff > 1) & (pos_diff <= 1_000_000)\n",
    "\n",
    "        filtered_group = group[mask]\n",
    "        \n",
    "        if filtered_group.empty:\n",
    "            continue\n",
    "        \n",
    "        edge_list.extend([(idx, other_idx) for idx, others in enumerate(mask) for other_idx in others.nonzero()[0]])\n",
    "        \n",
    "        edge_attr = np.vstack([filtered_group['nearest_genes'].values,\n",
    "                               filtered_group['#chrom'].values,\n",
    "                               filtered_group['pos'].values,\n",
    "                               filtered_group['af_alt'].values,\n",
    "                               filtered_group['af_alt_cases'].values,\n",
    "                               filtered_group['af_alt_controls'].values,\n",
    "                               np.exp(-np.log(2) / 100_000 * pos_diff[mask])]).T\n",
    "                               \n",
    "        edge_attributes.append(edge_attr)\n",
    "\n",
    "    edge_attributes = np.vstack(edge_attributes)\n",
    "    edge_attributes = scaler.fit_transform(edge_attributes)\n",
    "    \n",
    "    return torch.tensor(edge_list, dtype=torch.long).t().contiguous(), torch.tensor(edge_attributes, dtype=torch.float)\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_attr: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "\n",
    "def count_isolated_nodes(graph: Data) -> int:\n",
    "    node_degrees = graph.edge_index[0].bincount(minlength=graph.num_nodes)\n",
    "    isolated_nodes = (node_degrees == 0).sum().item()\n",
    "    return isolated_nodes\n",
    "\n",
    "\n",
    "# Create a profiler object\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "edges, edge_attr = preprocess_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, edges, edge_attr)\n",
    "graph.y = torch.tensor(data['finemapped'].values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "isolated_nodes = count_isolated_nodes(graph)\n",
    "print(f\"Number of isolated nodes: {isolated_nodes}\")\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 3 lines\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9677fc1-dd2d-4310-9f34-613f05f64ac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorch Geometric -> NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089ea21e-4f1c-48b2-93f0-04a71bbc268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 8214\n",
      "Number of edges: 403850\n",
      "Average degree: 98.33211589968347\n",
      "Number of connected components: 1\n",
      "Largest connected component (number of nodes): 8214\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def pyg_to_nx(graph):\n",
    "    # Initialize an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Extract edge indices and attributes\n",
    "    edge_index = graph.edge_index.t().tolist()\n",
    "    edge_attr = graph.edge_attr.tolist()\n",
    "\n",
    "    # Combine edge indices and attributes into a single list\n",
    "    edges = [(src, dst, {\"weight\": attr[0]}) for (src, dst), attr in zip(edge_index, edge_attr)]  # Change here: assume the first element of attr is the weight\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Extract node features and create a dictionary with node index as keys\n",
    "    node_features = {i: {f\"feature_{j}\": feat[j] for j in range(len(feat))} for i, feat in enumerate(graph.x.tolist())}  # Change here: convert list of lists to dict\n",
    "\n",
    "    # Set node attributes\n",
    "    nx.set_node_attributes(G, node_features)\n",
    "    \n",
    "    return G\n",
    "\n",
    "nx_graph = pyg_to_nx(graph)\n",
    "\n",
    "# Print the total number of nodes and edges in the graph\n",
    "print(f\"Number of nodes: {nx_graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {nx_graph.number_of_edges()}\")\n",
    "\n",
    "# Calculate average degree\n",
    "degrees = [degree for _, degree in nx_graph.degree()]\n",
    "average_degree = sum(degrees) / nx_graph.number_of_nodes()\n",
    "print(f\"Average degree: {average_degree}\")\n",
    "\n",
    "\n",
    "# Calculate and print the number of connected components\n",
    "num_connected_components = nx.number_connected_components(nx_graph)\n",
    "print(f\"Number of connected components: {num_connected_components}\")\n",
    "\n",
    "# Find and print the largest connected component\n",
    "largest_connected_component = max(nx.connected_components(nx_graph), key=len)\n",
    "print(f\"Largest connected component (number of nodes): {len(largest_connected_component)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a97ab-d8a0-4867-982a-11f809d38719",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Louvain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c8a4c9-e9de-4192-be8e-b24b2cdc882d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bad node degree (-36.765336364507675)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommunity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m community_louvain\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use the Louvain method for community detection\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m partition \u001b[38;5;241m=\u001b[39m \u001b[43mcommunity_louvain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print the number of communities and the nodes in each community\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of communities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(partition\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\community\\community_louvain.py:249\u001b[0m, in \u001b[0;36mbest_partition\u001b[1;34m(graph, partition, weight, resolution, randomize, random_state)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_partition\u001b[39m(graph,\n\u001b[0;32m    164\u001b[0m                    partition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m                    weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    166\u001b[0m                    resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m,\n\u001b[0;32m    167\u001b[0m                    randomize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    168\u001b[0m                    random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the partition of the graph nodes which maximises the modularity\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    (or try..) using the Louvain heuristices\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    >>> plt.show()\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     dendo \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dendrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mrandomize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_at_level(dendo, \u001b[38;5;28mlen\u001b[39m(dendo) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\community\\community_louvain.py:350\u001b[0m, in \u001b[0;36mgenerate_dendrogram\u001b[1;34m(graph, part_init, weight, resolution, randomize, random_state)\u001b[0m\n\u001b[0;32m    348\u001b[0m current_graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    349\u001b[0m status \u001b[38;5;241m=\u001b[39m Status()\n\u001b[1;32m--> 350\u001b[0m \u001b[43mstatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m status_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    352\u001b[0m __one_level(current_graph, status, weight, resolution, random_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\community\\community_status.py:53\u001b[0m, in \u001b[0;36mStatus.init\u001b[1;34m(self, graph, weight, part)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deg \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     52\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad node degree (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(deg)\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees[count] \u001b[38;5;241m=\u001b[39m deg\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgdegrees[node] \u001b[38;5;241m=\u001b[39m deg\n",
      "\u001b[1;31mValueError\u001b[0m: Bad node degree (-36.765336364507675)"
     ]
    }
   ],
   "source": [
    "from community import community_louvain\n",
    "\n",
    "# Use the Louvain method for community detection\n",
    "partition = community_louvain.best_partition(nx_graph, weight='weight')\n",
    "\n",
    "# print the number of communities and the nodes in each community\n",
    "print(f\"Number of communities: {len(set(partition.values()))}\")\n",
    "for i in set(partition.values()):\n",
    "    print(f\"Community {i}: {[nodes for nodes in partition.keys() if partition[nodes] == i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261b811-c831-40af-820f-af03ad56778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a9ef4-b3c2-4670-be6c-9375f0bea019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "clusters = kmeans.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a133b-9855-4056-bd05-30106d3e7e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb8e86-0c9e-463a-a2c2-57204f9c28a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "clusters = hc.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e556b25-5e86-4a6b-be7d-ccd72c441f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fae162-d43d-4681-8f74-d25fb32b6b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "clusters = dbscan.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48037339-5dbd-41b4-b4d6-63308217d087",
   "metadata": {
    "tags": []
   },
   "source": [
    "### grid_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8d7f2-4a15-472e-8135-512b4c1fd2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_cluster import grid_cluster\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assume pos_chrom is a tensor of shape [num_nodes, 2], \n",
    "# where the first column represents 'pos' and the second column represents '#chrom'\n",
    "pos_chrom = torch.tensor(np.stack([snp_features['pos'].values, snp_features['#chrom'].values])).t()\n",
    "\n",
    "# Define the size of the grid cells. You may want to adjust this according to your needs.\n",
    "size = torch.tensor([2e7, 1])  # Creates a grid with cell size defined according to your data distribution\n",
    "\n",
    "# Perform the grid clustering\n",
    "cluster = grid_cluster(pos_chrom, size)\n",
    "\n",
    "# Now, cluster is a tensor of size [num_nodes] where each element is the cluster index of the corresponding node\n",
    "print(cluster)\n",
    "\n",
    "# Get the 'pos' and '#chrom' data\n",
    "x = pos_chrom[:, 0].numpy()  # extract 'pos'\n",
    "y = pos_chrom[:, 1].numpy()  # extract '#chrom'\n",
    "\n",
    "# Convert cluster tensor to numpy for use with matplotlib\n",
    "cluster = cluster.numpy()\n",
    "\n",
    "num_clusters = len(np.unique(cluster))\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "# Create a colormap based on the number of unique clusters\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list('rainbow', plt.cm.rainbow(np.linspace(0, 1, len(np.unique(cluster)))))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(x, y, c=cluster, cmap=cmap, alpha=0.6)\n",
    "plt.colorbar(sc, label='Cluster Index')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Chromosome Number')\n",
    "plt.title('Grid Clustering of SNPs')\n",
    "\n",
    "# Modify y-ticks to represent actual chromosome numbers\n",
    "plt.yticks(range(1, int(y.max())+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9ec22-c065-4cc1-b86b-a75ffe3de3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
