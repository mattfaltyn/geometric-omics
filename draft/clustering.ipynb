{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.1\n",
      "Pandas version: 2.0.3\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.3.0\n",
      "Torch version: 2.0.1+cu117\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA RTX A6000 (cuda)\n",
      "CUDA version: 11.7\n",
      "Number of CUDA devices: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "from category_encoders import BinaryEncoder\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d6d595-4ef8-41d5-b0a5-dbdc9f2d9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in each column:\n",
      "#chrom                   0\n",
      "pos                      0\n",
      "ref                      0\n",
      "alt                      0\n",
      "rsids              1366396\n",
      "nearest_genes       727855\n",
      "pval                     0\n",
      "mlogp                    0\n",
      "beta                     0\n",
      "sebeta                   0\n",
      "af_alt                   0\n",
      "af_alt_cases             0\n",
      "af_alt_controls          0\n",
      "id                       0\n",
      "finemapped               0\n",
      "trait                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for total number of null values in each column\n",
    "null_counts = data.isnull().sum()\n",
    "\n",
    "print(\"Total number of null values in each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55024792-dddf-4ee4-9dbc-fbcf01ce0c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20170"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=0.001, random_state=42)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc4255-7a42-4172-a91d-4df08665fccc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located (int).\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph:\n",
    "\n",
    "- For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if the absolute difference between their positions (`pos`) is less than or equal to 1,000,000 and greater than 1 (no loops). Create edges between all pairs of SNPs within the 1,000,000 base distance threshold. The edge weight is determined by the following formula:\n",
    "     \n",
    "```\n",
    "weights = 1 * e^(-ln(2) / 100_000 * pos_diff_abs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abe533-6610-4f82-b8d6-533eb010858d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## pyg graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ef7f4e-3f36-4b3d-8cf7-6c1902f4747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import time\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import cProfile, pstats, io\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c335af9c-d688-4250-bbae-abee5fa2c9fa",
   "metadata": {},
   "source": [
    "from math import exp, log\n",
    "\n",
    "edge_weight_cutoff = 1e-3  # set the cutoff value\n",
    "\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls']\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    categorical_cols = ['ref', 'alt', 'nearest_genes']\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    #snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "    \n",
    "\n",
    "def preprocess_edges(data: pd.DataFrame, snp_to_idx: dict, chunk_size: int=100_000) -> torch.Tensor:\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['nearest_genes'] = label_encoder.fit_transform(data['nearest_genes'])\n",
    "\n",
    "    edge_list = []\n",
    "    edge_attributes = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        for i in range(0, len(group), chunk_size):\n",
    "            chunk = group.iloc[i: i + chunk_size]\n",
    "\n",
    "            # Broadcasting to create a matrix of differences\n",
    "            pos_diff = abs(chunk['pos'].values[:, None] - chunk['pos'].values)\n",
    "            mask = (pos_diff > 1) & (pos_diff <= 1_000_000)\n",
    "\n",
    "            filtered_chunk = chunk[mask]\n",
    "\n",
    "            if filtered_chunk.empty:\n",
    "                continue\n",
    "\n",
    "            new_edge_list = [(idx, other_idx) for idx, others in enumerate(mask) for other_idx in others.nonzero()[0]]\n",
    "            edge_list.extend(new_edge_list)\n",
    "\n",
    "            edge_attr = np.vstack([\n",
    "                np.exp(-np.log(2) / 100_000 * pos_diff[mask])\n",
    "            ]).T.astype(np.float32)\n",
    "\n",
    "            edge_attributes.extend(edge_attr.tolist())\n",
    "\n",
    "    edge_list = np.array(edge_list, dtype=int)\n",
    "    edge_attributes = scaler.fit_transform(np.array(edge_attributes, dtype=np.float32))\n",
    "\n",
    "    # Remove edges with negative weights\n",
    "    mask = edge_attributes >= edge_weight_cutoff\n",
    "    edge_list = edge_list[mask.reshape(-1), :]\n",
    "    edge_attributes = edge_attributes[mask]\n",
    "\n",
    "    return torch.tensor(edge_list, dtype=torch.long).t().contiguous(), torch.tensor(edge_attributes, dtype=torch.float)\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_attr: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "def count_isolated_nodes(graph: Data) -> int:\n",
    "    node_degrees = graph.edge_index[0].bincount(minlength=graph.num_nodes)\n",
    "    isolated_nodes = (node_degrees == 0).sum().item()\n",
    "    return isolated_nodes\n",
    "\n",
    "\n",
    "# Create a profiler object\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "edges, edge_attr = preprocess_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, edges, edge_attr)\n",
    "graph.y = torch.tensor(data['finemapped'].values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "isolated_nodes = count_isolated_nodes(graph)\n",
    "print(f\"Number of isolated nodes: {isolated_nodes}\")\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 3 lines\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df22b92-01b5-4225-814d-ad95b11b875a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b5640e1-b322-4c4f-96ed-f39654c78729",
   "metadata": {},
   "source": [
    "from torch_geometric.utils import degree\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_graph_stats(graph, features_list):\n",
    "    print(f\"Number of nodes: {graph.x.size(0)}\")\n",
    "    print(f\"Number of edges: {graph.edge_index.size(1)}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "    # Compute and print degree-related stats\n",
    "    degrees = degree(graph.edge_index[0].long(), num_nodes=graph.x.size(0))\n",
    "    average_degree = degrees.float().mean().item()\n",
    "    median_degree = np.median(degrees.numpy())\n",
    "    std_degree = degrees.float().std().item()\n",
    "\n",
    "    print(f\"Average Degree: {average_degree}\")\n",
    "    print(f\"Median Degree: {median_degree}\")\n",
    "    print(f\"Standard Deviation of Degree: {std_degree}\")\n",
    "\n",
    "    # Compute and print edge-related stats\n",
    "    average_edge_weight = graph.edge_attr.float().mean().item()\n",
    "    median_edge_weight = np.median(graph.edge_attr.numpy())\n",
    "    std_edge_weight = graph.edge_attr.float().std().item()\n",
    "    min_edge_weight = graph.edge_attr.float().min().item()\n",
    "    max_edge_weight = graph.edge_attr.float().max().item()\n",
    "\n",
    "    print(f\"Average Edge Weight: {average_edge_weight}\")\n",
    "    print(f\"Median Edge Weight: {median_edge_weight}\")\n",
    "    print(f\"Standard Deviation of Edge Weight: {std_edge_weight}\")\n",
    "    print(f\"Min Edge Weight: {min_edge_weight}\")\n",
    "    print(f\"Max Edge Weight: {max_edge_weight}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.x.size(0) * (graph.x.size(0) - 1) / 2\n",
    "    density = graph.edge_index.size(1) / num_possible_edges\n",
    "\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_mask = torch.isnan(graph.x)\n",
    "    nan_features = []\n",
    "    for feature_idx, feature_name in enumerate(features_list):\n",
    "        if nan_mask[:, feature_idx].any():\n",
    "            nan_features.append(feature_name)\n",
    "\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_features)\n",
    "\n",
    "    # Compute and print descriptive stats for node feature vectors\n",
    "    feature_stats = []\n",
    "    node_features = graph.x\n",
    "    for i, feature_name in enumerate(features_list):\n",
    "        feature = node_features[:, i]\n",
    "        mean = feature.float().mean().item()\n",
    "        median = np.median(feature.numpy())\n",
    "        std = feature.float().std().item()\n",
    "        minimum = feature.float().min().item()\n",
    "        maximum = feature.float().max().item()\n",
    "\n",
    "        feature_stats.append([feature_name, mean, median, std, minimum, maximum])\n",
    "\n",
    "    headers = [\"Feature\", \"Mean\", \"Median\", \"Standard Deviation\", \"Minimum\", \"Maximum\"]\n",
    "    print(\"\\nNode Feature Vector Descriptive Statistics:\")\n",
    "    print(tabulate(feature_stats, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Print graph stats\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, snp_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9677fc1-dd2d-4310-9f34-613f05f64ac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PyTorch Geometric -> NetworkX"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4566df8c-dc48-4b2d-ab38-364ccdfb141c",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "def pyg_to_nx(graph):\n",
    "    # Initialize an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Extract edge indices and attributes\n",
    "    edge_index = graph.edge_index.t().tolist()\n",
    "    edge_attr = graph.edge_attr.tolist()\n",
    "\n",
    "    # Combine edge indices and attributes into a single list\n",
    "    edges = [(src, dst, {\"weight\": attr}) for (src, dst), attr in zip(edge_index, edge_attr)]  # Fixed here\n",
    "\n",
    "    # Add edges to the graph\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Extract node features and create a dictionary with node index as keys\n",
    "    node_features = {i: {f\"feature_{j}\": feat[j] for j in range(len(feat))} for i, feat in enumerate(graph.x.tolist())}  # Change here: convert list of lists to dict\n",
    "\n",
    "    # Set node attributes\n",
    "    nx.set_node_attributes(G, node_features)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "nx_graph = pyg_to_nx(graph)\n",
    "\n",
    "# Print the total number of nodes and edges in the graph\n",
    "print(f\"Number of nodes: {nx_graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {nx_graph.number_of_edges()}\")\n",
    "\n",
    "# Calculate average degree\n",
    "degrees = [degree for _, degree in nx_graph.degree()]\n",
    "average_degree = sum(degrees) / nx_graph.number_of_nodes()\n",
    "print(f\"Average degree: {average_degree}\")\n",
    "\n",
    "\n",
    "# Calculate and print the number of connected components\n",
    "num_connected_components = nx.number_connected_components(nx_graph)\n",
    "print(f\"Number of connected components: {num_connected_components}\")\n",
    "\n",
    "# Find and print the largest connected component\n",
    "largest_connected_component = max(nx.connected_components(nx_graph), key=len)\n",
    "print(f\"Largest connected component (number of nodes): {len(largest_connected_component)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a97ab-d8a0-4867-982a-11f809d38719",
   "metadata": {
    "tags": []
   },
   "source": [
    "## nx graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a6ced3-1a94-4280-b3ec-7363bd5710d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae4add5-3aa9-45d1-8eeb-52349717a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.19 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_graph(data):\n",
    "    # Sort the data\n",
    "    data.sort_values(['#chrom', 'pos'], inplace=True)\n",
    "    \n",
    "    # Create the graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph in bulk using a dict comprehension\n",
    "    G.add_nodes_from({\n",
    "        row['id']: row.to_dict() \n",
    "        for _, row in data.iterrows()\n",
    "    })\n",
    "\n",
    "    # Define a function to calculate weights for given indices\n",
    "    def calculate_weights(pos_diffs):\n",
    "        mask = (pos_diffs > 1) & (pos_diffs <= 500_000)\n",
    "        indices = np.argwhere(mask)\n",
    "        unique_indices = indices[indices[:, 0] < indices[:, 1]]\n",
    "        unique_pos_diffs = pos_diffs[unique_indices[:, 0], unique_indices[:, 1]]\n",
    "        return unique_indices, 1 * np.exp(-np.log(2) / 100_000 * unique_pos_diffs)\n",
    "\n",
    "    # Iterate over each chromosome group\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        ids = group['id'].values\n",
    "        pos = group['pos'].values\n",
    "        \n",
    "        # Apply batch operation\n",
    "        chunk_size = 40_000\n",
    "        overlap = 5_000  # Define overlap size\n",
    "        num_chunks = math.ceil(len(pos) / chunk_size)\n",
    "        \n",
    "        for chunk in range(num_chunks):\n",
    "            start_idx = max(0, chunk * chunk_size - overlap)\n",
    "            end_idx = min((chunk + 1) * chunk_size + overlap, len(pos))\n",
    "            \n",
    "            # Calculate pairwise absolute differences in position within each chunk\n",
    "            chunk_pos = pos[start_idx:end_idx]\n",
    "            chunk_pos_diffs = distance.squareform(distance.pdist(chunk_pos[:, None], 'cityblock'))\n",
    "\n",
    "            # Calculate weights\n",
    "            unique_indices, unique_weights = calculate_weights(chunk_pos_diffs)\n",
    "\n",
    "            # Add the unique edges to the graph with weights\n",
    "            for (i, j), weight in zip(unique_indices, unique_weights):\n",
    "                node1 = ids[start_idx + i]\n",
    "                node2 = ids[start_idx + j]\n",
    "                if not G.has_edge(node1, node2):\n",
    "                    G.add_edge(node1, node2, weight=weight)\n",
    "\n",
    "        del ids, pos  # Delete to free memory\n",
    "        group = None  # Free memory\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "nx_graph = create_graph(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6002583e-edf3-4e90-8af0-f1e08f2c9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 20170\n",
      "Number of edges: 73653\n",
      "Number of connected components: 720\n",
      "Degree distribution statistics:\n",
      "Minimum degree: 0\n",
      "Maximum degree: 23\n",
      "Average degree: 11.5\n",
      "Edge weight statistics:\n",
      "Minimum weight: 0.031250000000000014\n",
      "Maximum weight: 0.9999098949262333\n",
      "Average weight: 0.283027426315929\n",
      "Standard deviation of weights: 0.25800881775614215\n",
      "Size of the largest connected component (nodes): 331\n",
      "Size of the largest connected component (edges): 1386\n"
     ]
    }
   ],
   "source": [
    "# Print basic graph statistics\n",
    "print(\"Number of nodes:\", nx_graph.number_of_nodes())\n",
    "print(\"Number of edges:\", nx_graph.number_of_edges())\n",
    "\n",
    "# Print number of connected components\n",
    "num_components = nx.number_connected_components(nx_graph)\n",
    "print(\"Number of connected components:\", num_components)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Get degree distribution\n",
    "degree_sequence = sorted([d for n, d in nx_graph.degree()], reverse=True)\n",
    "degree_counts = Counter(degree_sequence)\n",
    "\n",
    "# Print degree distribution statistics\n",
    "print(\"Degree distribution statistics:\")\n",
    "print(\"Minimum degree:\", min(degree_counts.keys()))\n",
    "print(\"Maximum degree:\", max(degree_counts.keys()))\n",
    "print(\"Average degree:\", sum(degree_counts.keys()) / len(degree_counts.keys()))\n",
    "\n",
    "# Get the edge weight data\n",
    "edge_weights = [data['weight'] for _, _, data in nx_graph.edges(data=True) if 'weight' in data]\n",
    "\n",
    "# Convert edge weights to floats\n",
    "edge_weights = [float(weight) for weight in edge_weights if weight != 'weight']\n",
    "\n",
    "# Print edge weight statistics\n",
    "print(\"Edge weight statistics:\")\n",
    "print(\"Minimum weight:\", min(edge_weights))\n",
    "print(\"Maximum weight:\", max(edge_weights))\n",
    "print(\"Average weight:\", np.mean(edge_weights))\n",
    "print(\"Standard deviation of weights:\", np.std(edge_weights))\n",
    "\n",
    "\n",
    "# Find the largest connected component\n",
    "largest_component = max(nx.connected_components(nx_graph), key=len)\n",
    "\n",
    "# Get the subgraph of the largest connected component\n",
    "largest_component_subgraph = nx_graph.subgraph(largest_component)\n",
    "\n",
    "# Print the size of the largest connected component\n",
    "print(\"Size of the largest connected component (nodes):\", largest_component_subgraph.number_of_nodes())\n",
    "print(\"Size of the largest connected component (edges):\", largest_component_subgraph.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d3fdd89-1334-4add-ba8c-502cbb9b851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nx_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(nx_graph, f)\n",
    "\n",
    "#with open('nx_graph.pkl', 'rb') as f:\n",
    "#    nx_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595678bf-6de5-42a5-9aa1-2095ad0af78a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dbe45-93d8-403f-a446-0e0d033e6566",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Louvain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762cefa9-5129-4049-b1db-92291983f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from community import community_louvain\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8a4c9-e9de-4192-be8e-b24b2cdc882d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect communities using Louvain method\n",
    "partition = community_louvain.best_partition(nx_graph, weight='weight')\n",
    "\n",
    "# Print number of communities\n",
    "print(f\"Number of communities: {len(set(partition.values()))}\")\n",
    "\n",
    "communities = defaultdict(list)\n",
    "\n",
    "for node, community in partition.items():\n",
    "    communities[community].append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c8201-9799-430b-86c7-dda48a51aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = set(partition.values())\n",
    "community_sizes = [list(partition.values()).count(x) for x in communities]\n",
    "\n",
    "plt.hist(community_sizes, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Community Size Distribution')\n",
    "plt.xlabel('Community Size')\n",
    "plt.ylabel('Number of Communities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864a294-2df4-42af-bd3a-3c681725bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "colors = sns.color_palette('pastel')[0:len(communities)]\n",
    "\n",
    "for (community, nodes), color in zip(communities.items(), colors):\n",
    "    degrees = [nx_graph.degree(n) for n in nodes]\n",
    "    sns.histplot(degrees, bins=10, kde=False, color=color, element=\"step\", stat=\"density\")\n",
    "\n",
    "plt.title('Degree Distribution per Community')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499585e-2879-4d1d-89f7-f576bda028fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Centrality measures per community\n",
    "# Calculating centrality measures for large graphs can be computationally expensive.\n",
    "# Therefore, you might want to select a subset of communities or nodes for this analysis.\n",
    "for community, nodes in list(communities.items())[:5]:  # Just the first 5 communities\n",
    "    subgraph = nx_graph.subgraph(nodes)\n",
    "    degree_centrality = nx.degree_centrality(subgraph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "    closeness_centrality = nx.closeness_centrality(subgraph)\n",
    "\n",
    "    print(f'Community {community}:')\n",
    "    print(f'Max degree centrality: {max(degree_centrality.items(), key=operator.itemgetter(1))}')\n",
    "    print(f'Max betweenness centrality: {max(betweenness_centrality.items(), key=operator.itemgetter(1))}')\n",
    "    print(f'Max closeness centrality: {max(closeness_centrality.items(), key=operator.itemgetter(1))}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c27f88-bc29-4e55-ac23-d09ca7154cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Community interconnectivity\n",
    "intra_edges = 0\n",
    "inter_edges = 0\n",
    "for u, v in nx_graph.edges():\n",
    "    if partition[u] == partition[v]:\n",
    "        intra_edges += 1\n",
    "    else:\n",
    "        inter_edges += 1\n",
    "\n",
    "print(f'Ratio of intra-community to inter-community edges: {intra_edges / inter_edges}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6b1a5-c2b2-4203-a99a-9734fdb9798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Community contribution to overall network structure\n",
    "community_edge_weights = dict()\n",
    "for community, nodes in communities.items():\n",
    "    subgraph = nx_graph.subgraph(nodes)\n",
    "    total_edge_weight = sum([data.get('weight', 1) for u, v, data in subgraph.edges(data=True)])\n",
    "    community_edge_weights[community] = total_edge_weight\n",
    "\n",
    "# Convert dictionary to DataFrame for better visualization and sort by the edge weight\n",
    "df = pd.DataFrame(list(community_edge_weights.items()), columns=['Community', 'Sum of edge weights'])\n",
    "df = df.sort_values('Sum of edge weights', ascending=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441749f8-2157-4327-b046-5d2c2d46a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Subgraph visualization\n",
    "# Select the first community for demonstration. You might want to select different or multiple communities for your analysis.\n",
    "community_to_draw = list(communities.keys())[0]  \n",
    "nodes = communities[community_to_draw]\n",
    "subgraph = nx_graph.subgraph(nodes)\n",
    "\n",
    "pos = nx.spring_layout(subgraph)\n",
    "nx.draw(subgraph, pos, with_labels=False, node_color='blue', node_size=30, edge_color='grey')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261b811-c831-40af-820f-af03ad56778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a9ef4-b3c2-4670-be6c-9375f0bea019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=23, random_state=0)\n",
    "clusters = kmeans.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a133b-9855-4056-bd05-30106d3e7e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb8e86-0c9e-463a-a2c2-57204f9c28a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "clusters = hc.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e556b25-5e86-4a6b-be7d-ccd72c441f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fae162-d43d-4681-8f74-d25fb32b6b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "clusters = dbscan.fit_predict(graph.x.numpy())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_data = tsne.fit_transform(graph.x.numpy())\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters)\n",
    "\n",
    "# Add a color bar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48037339-5dbd-41b4-b4d6-63308217d087",
   "metadata": {
    "tags": []
   },
   "source": [
    "### grid_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8d7f2-4a15-472e-8135-512b4c1fd2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_cluster import grid_cluster\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assume pos_chrom is a tensor of shape [num_nodes, 2], \n",
    "# where the first column represents 'pos' and the second column represents '#chrom'\n",
    "pos_chrom = torch.tensor(np.stack([snp_features['pos'].values, snp_features['#chrom'].values])).t()\n",
    "\n",
    "# Define the size of the grid cells. You may want to adjust this according to your needs.\n",
    "size = torch.tensor([2e7, 1])  # Creates a grid with cell size defined according to your data distribution\n",
    "\n",
    "# Perform the grid clustering\n",
    "cluster = grid_cluster(pos_chrom, size)\n",
    "\n",
    "# Now, cluster is a tensor of size [num_nodes] where each element is the cluster index of the corresponding node\n",
    "print(cluster)\n",
    "\n",
    "# Get the 'pos' and '#chrom' data\n",
    "x = pos_chrom[:, 0].numpy()  # extract 'pos'\n",
    "y = pos_chrom[:, 1].numpy()  # extract '#chrom'\n",
    "\n",
    "# Convert cluster tensor to numpy for use with matplotlib\n",
    "cluster = cluster.numpy()\n",
    "\n",
    "num_clusters = len(np.unique(cluster))\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "# Create a colormap based on the number of unique clusters\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list('rainbow', plt.cm.rainbow(np.linspace(0, 1, len(np.unique(cluster)))))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(x, y, c=cluster, cmap=cmap, alpha=0.6)\n",
    "plt.colorbar(sc, label='Cluster Index')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Chromosome Number')\n",
    "plt.title('Grid Clustering of SNPs')\n",
    "\n",
    "# Modify y-ticks to represent actual chromosome numbers\n",
    "plt.yticks(range(1, int(y.max())+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9ec22-c065-4cc1-b86b-a75ffe3de3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
